{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edbcf5c6-61ff-4cd4-ad94-679ee336fe14",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a regularization term in the cost function. The cost function for Lasso Regression is:\n",
    "\n",
    "Minimize\n",
    "(\n",
    "1\n",
    "2\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    ")\n",
    "Minimize( \n",
    "2n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣)\n",
    "\n",
    "where:\n",
    "\n",
    "𝑦\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the actual value.\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    "  is the predicted value.\n",
    "𝛽\n",
    "𝑗\n",
    "β \n",
    "j\n",
    "​\n",
    "  are the coefficients.\n",
    "𝜆\n",
    "λ is the regularization parameter.\n",
    "The key difference from other regression techniques, like Ordinary Least Squares (OLS) or Ridge Regression, is the L1 regularization term \n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣, which can shrink some coefficients to exactly zero, effectively performing variable selection.\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to shrink some coefficients to zero. This means it can effectively select a subset of the most relevant features, making the model simpler and potentially improving interpretability and generalization by reducing overfitting.\n",
    "\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "In Lasso Regression, the coefficients represent the relationship between each feature and the target variable, similar to other linear regression models. However, due to the L1 regularization, some coefficients may be exactly zero, indicating that the corresponding features are not important for predicting the target variable. Non-zero coefficients indicate the strength and direction of the relationship between the feature and the target.\n",
    "\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter \n",
    "𝜆\n",
    "λ.\n",
    "\n",
    "Small \n",
    "𝜆\n",
    "λ: Results in a model closer to ordinary least squares, with less regularization. Most or all coefficients will be non-zero.\n",
    "Large \n",
    "𝜆\n",
    "λ: Increases regularization, shrinking more coefficients to zero, which can lead to a sparser model with fewer features.\n",
    "The choice of \n",
    "𝜆\n",
    "λ balances bias and variance. A high \n",
    "𝜆\n",
    "λ can lead to high bias and low variance, while a low \n",
    "𝜆\n",
    "λ can lead to low bias and high variance.\n",
    "\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Lasso Regression itself is a linear model, but it can be adapted for non-linear problems by applying a transformation to the input features. This can be done using techniques such as:\n",
    "\n",
    "Polynomial features: Adding polynomial terms of the input features.\n",
    "Basis functions: Applying functions such as splines or wavelets to the input features.\n",
    "Kernel tricks: Using kernel methods to map the input features to a higher-dimensional space where a linear relationship might exist.\n",
    "After transforming the features, Lasso Regression can be applied to the transformed feature set.\n",
    "\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "The main difference between Ridge Regression and Lasso Regression lies in the type of regularization they use:\n",
    "\n",
    "Ridge Regression: Uses L2 regularization, adding a penalty equal to the sum of the squared coefficients to the cost function. It tends to shrink coefficients but not to zero, retaining all features.\n",
    "\n",
    "Minimize\n",
    "(\n",
    "1\n",
    "2\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    ")\n",
    "Minimize( \n",
    "2n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " )\n",
    "\n",
    "Lasso Regression: Uses L1 regularization, adding a penalty equal to the sum of the absolute values of the coefficients to the cost function. This can shrink some coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "Minimize\n",
    "(\n",
    "1\n",
    "2\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    ")\n",
    "Minimize( \n",
    "2n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣)\n",
    "\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features. The L1 regularization term in Lasso tends to select one of the correlated features and shrink the others to zero, effectively removing the redundant features and reducing multicollinearity.\n",
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "The optimal value of \n",
    "𝜆\n",
    "λ in Lasso Regression is typically chosen using cross-validation. The process involves:\n",
    "\n",
    "Splitting the data into training and validation sets.\n",
    "Training the Lasso model on the training set for a range of \n",
    "𝜆\n",
    "λ values.\n",
    "Evaluating the model performance on the validation set for each \n",
    "𝜆\n",
    "λ.\n",
    "Selecting the \n",
    "𝜆\n",
    "λ that provides the best performance based on a chosen metric (e.g., mean squared error, R-squared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f5a45-b9d1-4ee6-916f-2a3e7058e5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
