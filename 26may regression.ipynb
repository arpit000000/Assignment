{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7a5a29-9d23-45a2-a513-e67bab5b8d61",
   "metadata": {},
   "source": [
    "Q1. Difference between Simple Linear Regression and Multiple Linear Regression\n",
    "Simple Linear Regression: This model predicts the outcome of a dependent variable based on one independent variable. The relationship is modeled through a straight line (linear equation).\n",
    "\n",
    "Example: Predicting a person's weight based on their height.\n",
    "Weight\n",
    "=\n",
    "ğ›½\n",
    "0\n",
    "+\n",
    "ğ›½\n",
    "1\n",
    "â‹…\n",
    "Height\n",
    "Weight=Î² \n",
    "0\n",
    "â€‹\n",
    " +Î² \n",
    "1\n",
    "â€‹\n",
    " â‹…Height\n",
    "Multiple Linear Regression: This model predicts the outcome of a dependent variable based on two or more independent variables. The relationship is still linear, but it includes multiple predictors.\n",
    "\n",
    "Example: Predicting a person's weight based on their height and age.\n",
    "Weight\n",
    "=\n",
    "ğ›½\n",
    "0\n",
    "+\n",
    "ğ›½\n",
    "1\n",
    "â‹…\n",
    "Height\n",
    "+\n",
    "ğ›½\n",
    "2\n",
    "â‹…\n",
    "Age\n",
    "Weight=Î² \n",
    "0\n",
    "â€‹\n",
    " +Î² \n",
    "1\n",
    "â€‹\n",
    " â‹…Height+Î² \n",
    "2\n",
    "â€‹\n",
    " â‹…Age\n",
    "Q2. Assumptions of Linear Regression and How to Check Them\n",
    "Assumptions:\n",
    "\n",
    "Linearity: The relationship between the dependent and independent variables is linear.\n",
    "Check: Scatter plots or residual plots.\n",
    "Independence: The residuals are independent.\n",
    "Check: Durbin-Watson test.\n",
    "Homoscedasticity: The residuals have constant variance.\n",
    "Check: Residual plots.\n",
    "Normality: The residuals are normally distributed.\n",
    "Check: Q-Q plots or the Shapiro-Wilk test.\n",
    "No multicollinearity (for multiple regression): Independent variables are not highly correlated.\n",
    "Check: Variance Inflation Factor (VIF).\n",
    "Q3. Interpretation of Slope and Intercept\n",
    "Slope (\n",
    "ğ›½\n",
    "1\n",
    "Î² \n",
    "1\n",
    "â€‹\n",
    " ): The change in the dependent variable for a one-unit change in the independent variable.\n",
    "Intercept (\n",
    "ğ›½\n",
    "0\n",
    "Î² \n",
    "0\n",
    "â€‹\n",
    " ): The expected value of the dependent variable when all independent variables are zero.\n",
    "\n",
    "Example: In a model predicting salary based on years of experience:\n",
    "Salary\n",
    "=\n",
    "ğ›½\n",
    "0\n",
    "+\n",
    "ğ›½\n",
    "1\n",
    "â‹…\n",
    "Experience\n",
    "Salary=Î² \n",
    "0\n",
    "â€‹\n",
    " +Î² \n",
    "1\n",
    "â€‹\n",
    " â‹…Experience\n",
    "\n",
    "If \n",
    "ğ›½\n",
    "0\n",
    "=\n",
    "30\n",
    ",\n",
    "000\n",
    "Î² \n",
    "0\n",
    "â€‹\n",
    " =30,000 and \n",
    "ğ›½\n",
    "1\n",
    "=\n",
    "5\n",
    ",\n",
    "000\n",
    "Î² \n",
    "1\n",
    "â€‹\n",
    " =5,000:\n",
    "Intercept: A person with 0 years of experience is expected to earn $30,000.\n",
    "Slope: Each additional year of experience is associated with an increase of $5,000 in salary.\n",
    "Q4. Concept of Gradient Descent\n",
    "Gradient Descent: An optimization algorithm used to minimize the cost function in machine learning models. It iteratively adjusts the model parameters in the direction of the steepest descent of the cost function.\n",
    "\n",
    "Usage in Machine Learning:\n",
    "\n",
    "Initialize: Start with random values for model parameters.\n",
    "Compute Gradient: Calculate the gradient of the cost function with respect to the parameters.\n",
    "Update Parameters: Adjust the parameters by moving in the direction opposite to the gradient.\n",
    "Iterate: Repeat the process until the cost function converges to a minimum.\n",
    "Q5. Multiple Linear Regression Model\n",
    "Multiple Linear Regression: A model that predicts the outcome of a dependent variable based on multiple independent variables. It is an extension of simple linear regression.\n",
    "ğ‘Œ\n",
    "=\n",
    "ğ›½\n",
    "0\n",
    "+\n",
    "ğ›½\n",
    "1\n",
    "ğ‘‹\n",
    "1\n",
    "+\n",
    "ğ›½\n",
    "2\n",
    "ğ‘‹\n",
    "2\n",
    "+\n",
    "â€¦\n",
    "+\n",
    "ğ›½\n",
    "ğ‘›\n",
    "ğ‘‹\n",
    "ğ‘›\n",
    "+\n",
    "ğœ–\n",
    "Y=Î² \n",
    "0\n",
    "â€‹\n",
    " +Î² \n",
    "1\n",
    "â€‹\n",
    " X \n",
    "1\n",
    "â€‹\n",
    " +Î² \n",
    "2\n",
    "â€‹\n",
    " X \n",
    "2\n",
    "â€‹\n",
    " +â€¦+Î² \n",
    "n\n",
    "â€‹\n",
    " X \n",
    "n\n",
    "â€‹\n",
    " +Ïµ\n",
    "\n",
    "Difference from Simple Linear Regression:\n",
    "\n",
    "Simple Linear Regression: Only one independent variable.\n",
    "Multiple Linear Regression: Two or more independent variables.\n",
    "Q6. Multicollinearity in Multiple Linear Regression\n",
    "Multicollinearity: A situation where independent variables are highly correlated, making it difficult to isolate the effect of each predictor on the dependent variable.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Variance Inflation Factor (VIF): A VIF value greater than 10 indicates high multicollinearity.\n",
    "Correlation Matrix: Checking correlations between independent variables.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove Highly Correlated Predictors: Exclude one of the correlated variables.\n",
    "Principal Component Analysis (PCA): Transform variables into uncorrelated components.\n",
    "Regularization Techniques: Apply methods like Ridge Regression or Lasso Regression.\n",
    "Q7. Polynomial Regression Model\n",
    "Polynomial Regression: A type of regression where the relationship between the independent and dependent variable is modeled as an \n",
    "ğ‘›\n",
    "n-degree polynomial.\n",
    "ğ‘Œ\n",
    "=\n",
    "ğ›½\n",
    "0\n",
    "+\n",
    "ğ›½\n",
    "1\n",
    "ğ‘‹\n",
    "+\n",
    "ğ›½\n",
    "2\n",
    "ğ‘‹\n",
    "2\n",
    "+\n",
    "â€¦\n",
    "+\n",
    "ğ›½\n",
    "ğ‘›\n",
    "ğ‘‹\n",
    "ğ‘›\n",
    "+\n",
    "ğœ–\n",
    "Y=Î² \n",
    "0\n",
    "â€‹\n",
    " +Î² \n",
    "1\n",
    "â€‹\n",
    " X+Î² \n",
    "2\n",
    "â€‹\n",
    " X \n",
    "2\n",
    " +â€¦+Î² \n",
    "n\n",
    "â€‹\n",
    " X \n",
    "n\n",
    " +Ïµ\n",
    "\n",
    "Difference from Linear Regression:\n",
    "\n",
    "Linear Regression: Models a straight-line relationship.\n",
    "Polynomial Regression: Models a nonlinear relationship by including polynomial terms.\n",
    "Q8. Advantages and Disadvantages of Polynomial Regression\n",
    "Advantages:\n",
    "\n",
    "Flexibility: Can model more complex relationships than linear regression.\n",
    "Fit Curves: Better fit for data with curvature.\n",
    "Disadvantages:\n",
    "\n",
    "Overfitting: High-degree polynomials can overfit the data, capturing noise instead of the true relationship.\n",
    "Complexity: More complex to interpret and can lead to computational difficulties.\n",
    "Situations to Prefer Polynomial Regression:\n",
    "\n",
    "When the relationship between variables is known or suspected to be nonlinear.\n",
    "When a linear model does not adequately capture the trend in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adfa87d-f3de-4730-9e21-e7a4f59471ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
