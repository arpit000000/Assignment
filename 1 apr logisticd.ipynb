{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58f7b42-267a-462d-a6ca-dbcd9b3f37fa",
   "metadata": {},
   "source": [
    "\n",
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate.\n",
    "Linear Regression:\n",
    "\n",
    "Purpose: Predicts a continuous dependent variable based on one or more independent variables.\n",
    "Model: \n",
    "𝑦\n",
    "^\n",
    "=\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "⋯\n",
    "+\n",
    "𝛽\n",
    "𝑝\n",
    "𝑥\n",
    "𝑝\n",
    "y\n",
    "^\n",
    "​\n",
    " =β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +⋯+β \n",
    "p\n",
    "​\n",
    " x \n",
    "p\n",
    "​\n",
    " \n",
    "Cost Function: Mean Squared Error (MSE).\n",
    "Logistic Regression:\n",
    "\n",
    "Purpose: Predicts a binary dependent variable (0 or 1) based on one or more independent variables.\n",
    "Model: Uses the logistic function to model the probability that the dependent variable equals a certain value (usually 1).\n",
    "𝑃\n",
    "(\n",
    "𝑦\n",
    "=\n",
    "1\n",
    "∣\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "𝑒\n",
    "−\n",
    "(\n",
    "𝛽\n",
    "0\n",
    "+\n",
    "𝛽\n",
    "1\n",
    "𝑥\n",
    "1\n",
    "+\n",
    "𝛽\n",
    "2\n",
    "𝑥\n",
    "2\n",
    "+\n",
    "⋯\n",
    "+\n",
    "𝛽\n",
    "𝑝\n",
    "𝑥\n",
    "𝑝\n",
    ")\n",
    "P(y=1∣x)= \n",
    "1+e \n",
    "−(β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    "​\n",
    " +⋯+β \n",
    "p\n",
    "​\n",
    " x \n",
    "p\n",
    "​\n",
    " )\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "Cost Function: Log-Loss or Binary Cross-Entropy.\n",
    "Example Scenario: Logistic regression would be more appropriate in scenarios where the outcome is categorical. For instance, predicting whether a patient has a certain disease (yes/no) based on their medical history and test results.\n",
    "\n",
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n",
    "The cost function used in logistic regression is the Log-Loss or Binary Cross-Entropy:\n",
    "\n",
    "Cost\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    ")\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "𝑛\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "[\n",
    "𝑦\n",
    "𝑖\n",
    "log\n",
    "⁡\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "𝑦\n",
    "𝑖\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    "𝑖\n",
    ")\n",
    ")\n",
    "]\n",
    "Cost(h \n",
    "θ\n",
    "​\n",
    " (x),y)=− \n",
    "n\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " [y \n",
    "i\n",
    "​\n",
    " log(h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))+(1−y \n",
    "i\n",
    "​\n",
    " )log(1−h \n",
    "θ\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ))]\n",
    "\n",
    "where:\n",
    "\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "h \n",
    "θ\n",
    "​\n",
    " (x) is the predicted probability that \n",
    "𝑦\n",
    "=\n",
    "1\n",
    "y=1.\n",
    "𝑦\n",
    "y is the actual label (0 or 1).\n",
    "Optimization:\n",
    "\n",
    "Gradient Descent: The most common optimization technique, iteratively updating the model parameters to minimize the cost function.\n",
    "Variants: Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent.\n",
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "Regularization adds a penalty to the cost function to prevent overfitting by discouraging complex models:\n",
    "\n",
    "L1 Regularization (Lasso): Adds the sum of the absolute values of the coefficients.\n",
    "Cost\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    ")\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "Cost(h \n",
    "θ\n",
    "​\n",
    " (x),y)+λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "L2 Regularization (Ridge): Adds the sum of the squared values of the coefficients.\n",
    "Cost\n",
    "(\n",
    "ℎ\n",
    "𝜃\n",
    "(\n",
    "𝑥\n",
    ")\n",
    ",\n",
    "𝑦\n",
    ")\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    "Cost(h \n",
    "θ\n",
    "​\n",
    " (x),y)+λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " \n",
    "Regularization helps in:\n",
    "\n",
    "Reducing Model Complexity: Penalizes large coefficients, resulting in a simpler model.\n",
    "Preventing Overfitting: Helps the model generalize better to new, unseen data.\n",
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?\n",
    "ROC Curve (Receiver Operating Characteristic Curve) plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings:\n",
    "\n",
    "TPR (Sensitivity): \n",
    "TPR\n",
    "=\n",
    "TP\n",
    "TP\n",
    "+\n",
    "FN\n",
    "TPR= \n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " \n",
    "FPR: \n",
    "FPR\n",
    "=\n",
    "FP\n",
    "FP\n",
    "+\n",
    "TN\n",
    "FPR= \n",
    "FP+TN\n",
    "FP\n",
    "​\n",
    " \n",
    "Evaluation:\n",
    "\n",
    "Area Under the Curve (AUC): Measures the overall performance. An AUC of 1 represents a perfect model, while an AUC of 0.5 represents a random model.\n",
    "Interpretation: The closer the curve is to the top-left corner, the better the model's performance.\n",
    "Q5. What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?\n",
    "Common Techniques:\n",
    "\n",
    "Filter Methods: Use statistical measures to score and select features (e.g., Chi-square test, ANOVA).\n",
    "Wrapper Methods: Evaluate feature subsets using a model and select the best performing subset (e.g., Recursive Feature Elimination).\n",
    "Embedded Methods: Perform feature selection during model training (e.g., Lasso regression, Elastic Net).\n",
    "Improvement:\n",
    "\n",
    "Reduces Overfitting: By selecting only relevant features, it reduces the risk of the model capturing noise.\n",
    "Improves Interpretability: Simpler models with fewer features are easier to understand and interpret.\n",
    "Enhances Performance: Focusing on the most important features can improve the model's predictive power.\n",
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?\n",
    "Strategies:\n",
    "\n",
    "Resampling:\n",
    "Oversampling: Increase the number of instances in the minority class (e.g., SMOTE).\n",
    "Undersampling: Decrease the number of instances in the majority class.\n",
    "Class Weights: Adjust the weights of the classes in the cost function to penalize misclassifications of the minority class more heavily.\n",
    "Synthetic Data Generation: Create synthetic samples for the minority class (e.g., SMOTE).\n",
    "Algorithmic Approaches: Use algorithms designed for imbalanced datasets (e.g., Balanced Random Forest).\n",
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?\n",
    "Common Issues:\n",
    "\n",
    "Multicollinearity: When independent variables are highly correlated.\n",
    "Solution: Use regularization (Lasso or Ridge), remove correlated features, or use Principal Component Analysis (PCA) to reduce dimensionality.\n",
    "Non-linearity: Logistic regression assumes a linear relationship between the log-odds and the predictors.\n",
    "Solution: Include polynomial or interaction terms, or use non-linear models like decision trees.\n",
    "Outliers: Outliers can unduly influence the model.\n",
    "Solution: Detect and remove outliers, or use robust regression techniques.\n",
    "Imbalanced Datasets: Can lead to biased models.\n",
    "Solution: Use resampling techniques, adjust class weights, or use appropriate metrics like precision, recall, and F1-score for evaluation.\n",
    "Overfitting: Model performs well on training data but poorly on test data.\n",
    "Solution: Use regularization, cross-validation, and simpler models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
