{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f907e69b-446f-48a9-9977-ad37699a6400",
   "metadata": {},
   "source": [
    "\n",
    "Q1. Concept of R-squared in Linear Regression Models\n",
    "R-squared (RÂ²): A statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model.\n",
    "\n",
    "Calculation:\n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "SS\n",
    "res\n",
    "SS\n",
    "tot\n",
    "R \n",
    "2\n",
    " =1âˆ’ \n",
    "SS \n",
    "tot\n",
    "â€‹\n",
    " \n",
    "SS \n",
    "res\n",
    "â€‹\n",
    " \n",
    "â€‹\n",
    " \n",
    "\n",
    "SS\n",
    "res\n",
    "SS \n",
    "res\n",
    "â€‹\n",
    "  (Residual Sum of Squares): The sum of the squared differences between the observed and predicted values.\n",
    "SS\n",
    "tot\n",
    "SS \n",
    "tot\n",
    "â€‹\n",
    "  (Total Sum of Squares): The sum of the squared differences between the observed values and the mean of the observed values.\n",
    "Interpretation:\n",
    "\n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "0\n",
    "R \n",
    "2\n",
    " =0: The model explains none of the variance.\n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "R \n",
    "2\n",
    " =1: The model explains all of the variance.\n",
    "Values between 0 and 1 indicate the proportion of the variance explained by the model.\n",
    "Q2. Definition of Adjusted R-squared\n",
    "Adjusted R-squared: A modified version of R-squared that adjusts for the number of predictors in the model. It accounts for the potential overestimation of R-squared when adding more predictors.\n",
    "\n",
    "Calculation:\n",
    "AdjustedÂ \n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "(\n",
    "1\n",
    "âˆ’\n",
    "ğ‘…\n",
    "2\n",
    ")\n",
    "(\n",
    "ğ‘›\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "ğ‘›\n",
    "âˆ’\n",
    "ğ‘\n",
    "âˆ’\n",
    "1\n",
    "AdjustedÂ R \n",
    "2\n",
    " =1âˆ’ \n",
    "nâˆ’pâˆ’1\n",
    "(1âˆ’R \n",
    "2\n",
    " )(nâˆ’1)\n",
    "â€‹\n",
    " \n",
    "\n",
    "ğ‘›\n",
    "n: Number of observations.\n",
    "ğ‘\n",
    "p: Number of predictors.\n",
    "Difference from R-squared:\n",
    "\n",
    "Adjusted R-squared penalizes the addition of non-significant predictors, thus providing a more accurate measure of model performance when multiple predictors are involved.\n",
    "Q3. When to Use Adjusted R-squared\n",
    "Adjusted R-squared is more appropriate when comparing the performance of regression models with different numbers of predictors. It accounts for model complexity and avoids overestimating the explanatory power of the model by adjusting for the number of predictors.\n",
    "\n",
    "Q4. RMSE, MSE, and MAE in Regression Analysis\n",
    "Mean Squared Error (MSE): The average of the squared differences between the observed and predicted values.\n",
    "MSE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " ) \n",
    "2\n",
    " \n",
    "Root Mean Squared Error (RMSE): The square root of the MSE, providing a measure in the same units as the dependent variable.\n",
    "RMSE\n",
    "=\n",
    "MSE\n",
    "RMSE= \n",
    "MSE\n",
    "â€‹\n",
    " \n",
    "Mean Absolute Error (MAE): The average of the absolute differences between the observed and predicted values.\n",
    "MAE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ£\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    "âˆ£\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " âˆ£y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " âˆ£\n",
    "Representation:\n",
    "\n",
    "MSE and RMSE: Penalize larger errors more than smaller errors due to squaring the residuals.\n",
    "MAE: Provides a more direct measure of average error magnitude.\n",
    "Q5. Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "RMSE:\n",
    "Advantages: Sensitive to larger errors, useful when large errors are particularly undesirable.\n",
    "Disadvantages: Can be overly influenced by outliers.\n",
    "MSE:\n",
    "Advantages: Similar to RMSE but in squared units, useful for theoretical derivations.\n",
    "Disadvantages: Interpretation is less intuitive due to squared units.\n",
    "MAE:\n",
    "Advantages: Simple to understand, less sensitive to outliers compared to RMSE and MSE.\n",
    "Disadvantages: Does not penalize larger errors as much as RMSE and MSE.\n",
    "Q6. Concept of Lasso Regularization\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regularization: A type of regularization that adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.\n",
    "LossÂ Function\n",
    "=\n",
    "RSS\n",
    "+\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "âˆ£\n",
    "ğ›½\n",
    "ğ‘—\n",
    "âˆ£\n",
    "LossÂ Function=RSS+Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " âˆ£Î² \n",
    "j\n",
    "â€‹\n",
    " âˆ£\n",
    "\n",
    "Difference from Ridge Regularization: Ridge adds a penalty equal to the square of the magnitude of coefficients.\n",
    "RidgeÂ LossÂ Function\n",
    "=\n",
    "RSS\n",
    "+\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "2\n",
    "RidgeÂ LossÂ Function=RSS+Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "2\n",
    "â€‹\n",
    " \n",
    "Appropriate Use: Lasso is more appropriate when feature selection is desired, as it can shrink some coefficients to zero, effectively removing them from the model.\n",
    "\n",
    "Q7. Regularized Linear Models and Overfitting\n",
    "Regularized Linear Models: These models add a penalty to the loss function to constrain the magnitude of the coefficients, thus reducing model complexity and preventing overfitting.\n",
    "\n",
    "Example: Ridge and Lasso regression.\n",
    "Illustration:\n",
    "\n",
    "Without regularization, a model might overfit the training data, capturing noise and leading to poor generalization on new data.\n",
    "With regularization, the model simplifies, potentially ignoring irrelevant features and providing better generalization.\n",
    "Q8. Limitations of Regularized Linear Models\n",
    "Bias-Variance Tradeoff: Regularization introduces bias to reduce variance, which might lead to underfitting if the penalty is too strong.\n",
    "Choice of Regularization Parameter: Requires careful tuning of the regularization parameter (\n",
    "ğœ†\n",
    "Î»).\n",
    "Not Always the Best Choice: Regularization may not be effective if the true model is complex and requires many features.\n",
    "Q9. Comparing Regression Models Using RMSE and MAE\n",
    "Model A (RMSE = 10): Indicates that the average magnitude of the error is 10.\n",
    "Model B (MAE = 8): Indicates that the average absolute error is 8.\n",
    "Choice: If minimizing average error magnitude is the goal, Model B might be preferred. However, the choice depends on the context:\n",
    "\n",
    "RMSE: If larger errors are more undesirable, Model A might be reconsidered despite a higher RMSE.\n",
    "Limitations: RMSE and MAE measure different aspects; RMSE is more sensitive to outliers than MAE.\n",
    "Q10. Comparing Regularized Linear Models\n",
    "Model A (Ridge, \n",
    "ğœ†\n",
    "=\n",
    "0.1\n",
    "Î»=0.1)\n",
    "Model B (Lasso, \n",
    "ğœ†\n",
    "=\n",
    "0.5\n",
    "Î»=0.5)\n",
    "Choice:\n",
    "\n",
    "Model A (Ridge): Better if all features are believed to have some predictive power and multicollinearity is a concern.\n",
    "Model B (Lasso): Better if feature selection is desired and some features are expected to be irrelevant.\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Ridge: Does not perform feature selection; all coefficients are shrunk but none are eliminated.\n",
    "Lasso: Can eliminate irrelevant features, but may be less stable in the presence of correlated predictors compared to Ridge.\n",
    "Choice of \n",
    "ğœ†\n",
    "Î»: Both models require tuning the regularization parameter to balance bias and variance effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72483208-9753-403e-880d-b89a2f4b3875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
